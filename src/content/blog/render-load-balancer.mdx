---
title: The curious case of a slow Render load balancer
generated: 1701894028870
description: How we removed 50ms of delay on every inbound request
pubDate: Aug 21, 2024
author: Max McDonnell
---

import LatencyHistogram from './render-load-balancer/LatencyHistogram.astro';

Last year we migrated from [Supabase to
Render](https://blog.val.town/blog/migrating-from-supabase/). Render
has been excellent: our development velocity has gone up, we have realistic
full-stack development environments, and the tools for monitoring
and configuration have been rock-solid. Sometimes we log into the AWS
console for something unrelated and feel blessed to be insulated from the
see of orange buttons and indecipherable error states.

Exceptâ€¦

While we were figuring out how to [improve the speed of HTTP
Vals](https://github.com/val-town/val-town-product/discussions/113), we noticed
that there was a surprising amount of latency between us and our our Render
servers. The latency was spiky, but we think on average it was adding
about 50ms to requests we sent through Render.

You can try it out yourself: I've made two Vals, [one that works using our new
load balancer](https://www.val.town/v/maxm/itsOnValTown) (like all Vals), and
[another one](https://www.val.town/v/maxm/itsOnRender) that I've set up to send
through Render's load balancer.

I even had [Townie](https://www.val.town/townie) whip up [a little
tool](https://maxm-governingvioletkrill.web.val.run/) to display the performance
difference between the two.

[![a performance tool to test both urls generated by
townie](./render-load-balancer/townie-performance-test.png)](https://maxm-governingvioletkrill.web.val.run/)

You'll see that generally they both do pretty well, but Render is a little
slower and sometimes has much higher tail latencies. I think because this tool
makes pre-warmed requests from within a datacenter you will not always see the
50ms slowdown we measured, but that is what we were seeing in more
representative testing of user code.

*Render has mentioned that they're working on improving this, and it certainly
seems better than when I was first testing, so maybe things will change in the
future.*

## Don't wrap your FaaS in a FaaS

We eventually learned that Render routes
every incoming request through a Cloudflare Worker: that's where the latency
is coming from.

So we're not just seeing the
geographic cost of hopping between the user, Cloudflare, and Render's server
instances in AWS, but we're also seeing the
occasional cold start or whatever scheduling congestion might be involved in
finding available capacity. There could also be a slowdown due to the Worker
code itself, if Render is making some datastore query for each request.
What we do know is that the slowdown is consistent and we can't
introspect it, or improve it ourselves.

We're building a [FaaS](https://en.wikipedia.org/wiki/Function_as_a_service)! We
can't wrap our FaaS in another FaaS. We have cold starts and scheduling and
worker re-use and resource problems to deal with ourselves. If we're fighting to
reduce tail latencies and Cloudflare Workers have their own tail latencies,
then we have less control over our performance than we'd like.

We talked to Render. They were friendly, helpful, and they read my benchmarks. They
confirmed the issue, but where unable to remove their Cloudflare layer or
help us find out a way to get around it.

What do we do next?

## Brainstorming

At this point we have a few considerations about Render:

1. We'd prefer not to route our requests through Cloudflare Workers.
2. Due to some quirks with "double Cloudflare" we cannot run our own Cloudflare
   account in front of Render's Cloudflare.
3. Render has other limitations, mainly limited Linux capabilities and lack of
   more advanced networking functionality.

How do we address these issues? We considered moving away entirely. We replicated
our application stack in AWS
and began talking with other providers that would provide us the level of
control we needed. In the end, it was hard to justify the migration cost and
difficult to stomach adding any complexity to our deploy and administration
workflows.

Well, can we sidestep their networking setup entirely? I first experimented with
Cloudflare Tunnel and was able to connect our Render server directly to
Cloudflare, skipping Render's load balancer. Requests flowed and I was able to
get a very good sense of the Render slowdown for the first time.

If Cloudflare Tunnel works, what about [Tailscale](https://tailscale.com/)
and [Wireguard](https://www.wireguard.com/)? Thankfully, Render
has an [example tailscale subnet
router](https://github.com/render-examples/tailscale). I got that running in
Render, set up an EC2 instance in AWS, grabbed the local IP address of one of
our Render servers, crossed my fingers and ran `tailscale ping 10.207.5.44`.

```
$ tailscale ping 10.207.5.44
pong from render-subnet-router-1 (10.207.5.44) via DERP(ord) in 113ms
pong from render-subnet-router-1 (10.207.5.44) via DERP(ord) in 59ms
pong from render-subnet-router-1 (10.207.5.44) via DERP(ord) in 60ms
pong from render-subnet-router-1 (10.207.5.44) via 3.134.238.10:49446 in 1ms
```

Yay! After a few [DERP](https://tailscale.com/kb/1232/derp-servers) hops we can
directly connect between Render's servers and our EC2 instance, both within
us-east-2. This confirms that we can route traffic from our own AWS instances to
Render's servers directly. With this as a building block we could build whatever
networking setup we'd like, and we can defer the decision to move on from
Render.

Can we use this? Should we send all of our production traffic over Tailscale?
Will it be reliable? What should our load balancer look like in AWS?

## Putting it all together

Yes, we can use this, it's been working well. Here's the architecture we ended up with.

![](./render-load-balancer/arch-diagram.png)

We have an Application Load Balancer (ALB) doing TLS termination and handling
incoming traffic. We have [HAProxy](https://www.haproxy.org/) servers that receive
that traffic and route it
to individual hosts through the subnet routers. We have redundant load balancers
and subnet routers and we're resilient to many types of failures.

A few notes on this setup:

1. HAProxy needs to know where to route traffic, but the Render server IPs change
   every deploy. Thankfully Render provides a DNS server that reports the server
   addresses. We configure HAProxy to listen for DNS changes and update the
   routing table accordingly.
2. Tailscale subnet routers can be run redundantly. If one of them goes down,
   we'll simply drop those TCP packets and traffic will be seamlessly re-routed
   through the other instance.
3. If an HAProxy server crashes, we lose any connections that are being
   served, but we've placed those instances in an Auto Scaling Group so a new
   instance will be created to replace it.

This setup has been working well. All inbound HTTP requests are quicker
and we've seen a reduction in tail latencies. We deploy these instances very
infrequently so the administrative overhead is relatively low.

We will need to keep an eye on the network volume in these instances and adjust
accordingly. We also need to be very careful that we don't end up routing
traffic over a Tailscale DERP relay or we'll be paying the cost of bouncing to a
different datacenter. And yes, we are now running our own load balancer, which
would have been nice to avoid. It is nice that we have HAProxy in the mix
though, we've already been taking advantage of its many features.

### In closing

<LatencyHistogram />

To take one final moment of comparison, here is the latency distribution during
some recent testing of each load balancing setup. It is too bad that this delay
was present at all, but it's nice to be clear of it.

Our march to better performance has been going well and we are well clear of the
numbers originally reported [in this
discussion](https://github.com/val-town/val-town-product/discussions/113). In a
future post we'll summarize how we've been improving performance more broadly, and
how we'll continue into the future!
